---
title: "Modeling"
format: html
editor: visual
---


#Basic Introduction 

#Split data into testing and training data 

```{r}
library(tidyverse)
library(tidymodels)
library(yardstick)
library(ggplot2)
library(glmnet)
library(tree)
library(baguette)
library(parsnip)
library(ranger)
```



```{r}
#Reload the data 
#Read in data and create a tibble 
diabetes_data <- read.csv("diabetes_binary_health_indicators_BRFSS2015.csv")
diabetes_data <- as_tibble(diabetes_data)

#Create vectors for the factor variables 
Diabetes_labels <- c("No Diabetes", "Diabetes")
HighBP_labels <- c("Not High BP", "High BP")
HighChol_labels <- c("Not High Chol", "Chol")
PhysActivity_labels <- c("No","Yes")
Sex_labels <- c("Female","Male")
Age_labels <- c("18-24","25-29","30-34","35-59",
                "40-44","45-49","50-54","55-59",
                "60-64", "65-69", "70-74", "75-79","80 or older")
Income_levels <-c("Less than $10,000",">15000",">20000",">25000",">35000",">50000",
                  ">75000","75000+")



#Mutating the data set to only include the relevant variables models ran and to modify 
#the type of variable 
diabetes_data <- diabetes_data |>
  drop_na() |>
  mutate(
    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = Diabetes_labels),
    HighBP = factor(HighBP, levels = c(0,1), labels = HighBP_labels),
    HighChol = factor(HighChol, levels = c(0,1), labels = HighChol_labels),
    PhysActivity = factor(PhysActivity, levels = c(0,1), labels = PhysActivity_labels),
    Sex = factor(Sex, levels = c(0,1), labels = Sex_labels),
    Age = factor(Age, levels = 1:13, labels = Age_labels),
    Income = factor(Income, levels = 1:8, labels = Income_levels)
  ) |>
  select(Diabetes_binary,HighBP,HighChol,PhysActivity,Sex,Age,Income,BMI)
```


```{r}
set.seed(123)

model_split <- initial_split(diabetes_data,prop=.7)
test <- testing(model_split)
train <- training(model_split)
diabetes_CV_folds <- vfold_cv(train, 5)
```

#Logistic Regression Model 

```{r}
#Model 1: BMI HighBP and PhysActivity
LR1 <- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |>
  step_normalize(BMI)|>
  step_dummy(HighBP, PhysActivity)

#Model 2: BMI HighBP and PhysActivity and interaction term between
#BMI and highBP 
LR2 <- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |>
  step_dummy(HighBP, PhysActivity)|>
  step_normalize(all_predictors())|> 
  step_interact(~ BMI:starts_with("HighBP_"))

#Model 3: BMI HighBP and PhysActivity and interaction term between
#BMI and PhysActivity 
LR3 <- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |>
  step_dummy(HighBP, PhysActivity)|>
  step_normalize(all_predictors())|> 
  step_interact(~ BMI:starts_with("PhysActivity_"))

#Set model type and engine 
LR_spec <- logistic_reg() |>
 set_engine("glm")

#Create workflows
LR1_wkf <- workflow() |>
 add_recipe(LR1) |>
 add_model(LR_spec)
LR2_wkf <- workflow() |>
 add_recipe(LR2) |>
 add_model(LR_spec)
LR3_wkf <- workflow() |>
 add_recipe(LR3) |>
 add_model(LR_spec)

#Fit to CV folds 
LR1_fit <- LR1_wkf |>
 fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))
LR2_fit <- LR2_wkf |>
 fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))
LR3_fit <- LR3_wkf |>
 fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))

rbind(LR1_fit |> collect_metrics(),
 LR2_fit |> collect_metrics(),
 LR3_fit |> collect_metrics()) |>
 mutate(Model = c("Model1", "Model1", "Model2", "Model2", "Model3", "Model3")) |>
 select(Model, everything())

mean(train$Diabetes_binary=="Diabetes")

```

#Classification Tree

```{r}
#Create Recipe 
tree_rec <- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |>
  step_normalize(BMI)|>
  step_dummy(all_nominal_predictors())

tree_rec

tree_mod <- decision_tree(tree_depth = tune(),
  min_n = 20,
  cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

#Create workflow 
tree_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(tree_mod)

#Set up my own tuning grid
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(10, 5))

tree_fits <- tree_wkf |> 
  tune_grid(resamples = diabetes_CV_folds,
            grid = tree_grid,
            metrics=metric_set(accuracy, mn_log_loss))

tree_fits |>
  collect_metrics() 

tree_fits %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)

tree_fits |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)


tree_best_params <- select_best(tree_fits, metric = "mn_log_loss")
tree_best_params

tree_final_wkf <- tree_wkf |>
  finalize_workflow(tree_best_params)

tree_final_fit <- tree_final_wkf |>
  last_fit(model_split,metrics = metric_set(accuracy, mn_log_loss))
tree_final_fit

tree_final_fit |>
  collect_metrics()


```



#Random Forest 
```{r}

ran_forest_rec <- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |>
  step_normalize(BMI)|>
  step_dummy(all_nominal_predictors())

ran_forest_rec


rf_spec <- rand_forest(mtry = tune()) |>
 set_engine("ranger") |>
 set_mode("classification")

rf_wkf <- workflow() |>
 add_recipe(ran_forest_rec) |>
 add_model(rf_spec)

rf_fit <- rf_wkf |>
 tune_grid(resamples = diabetes_CV_folds,
 grid = 7,
 metrics = metric_set(accuracy, mn_log_loss))

rf_fit |>
 collect_metrics() |>
 filter(.metric == "mn_log_loss") |>
 arrange(mean)

rf_best_params <- select_best(rf_fit, metric = "mn_log_loss")
rf_best_params

rf_final_wkf <- rf_wkf |>
 finalize_workflow(rf_best_params)

rf_final_fit <- rf_final_wkf |>
 last_fit(model_split, metrics = metric_set(accuracy, mn_log_loss))

```

#Final Model Selection 
```{r}
tree_final_fit <- tree_final_wkf |>
  last_fit(model_split,metrics = metric_set(accuracy, mn_log_loss))
tree_final_fit|> collect_metrics()

rf_final_fit <- rf_final_wkf |>
 last_fit(model_split, metrics = metric_set(accuracy, mn_log_loss))
rf_final_fit |> collect_metrics()


LR3_final_fit <- LR3_wkf |>
 last_fit(model_split, metrics = metric_set(accuracy, mn_log_loss)) 

tree_final_fit|> collect_metrics()
rf_final_fit |> collect_metrics()
LR3_final_fit |> collect_metrics()

rbind(tree_final_fit |> collect_metrics(),
rf_final_fit |> collect_metrics(),
 LR3_final_fit |> collect_metrics()) |>
 mutate(Model = c("Classification Model", "Classification Model", "Random Forest", "Random Forest", "Logistic Regression", "Logistic Regression")) |>
 select(Model, everything())


final_model <- rf_final_fit$.workflow[[1]]

```

Declare Random Forest the Overall winner!! 



