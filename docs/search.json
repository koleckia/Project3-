[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "#Introduction section\nFor this project, I will be analyzing the impact of specific health indicators on the likelhood of getting diabetes. While there are many indicators in the data set, I have decided to only look into three specific variables: Physical activity, BMI and Blood Pressure. I picked these three variables to specifically narrow down the impacts of cardiometabolic factors on diabetes.\nFor the next section of the project, I will do exploratory data analysis to analyze the relationships each of the three variables have on the independent variable of having diabetes. This EDA will also help identify any data quality issues and help understand the different data types and distributions.\nIn the second part of this project, I will use three different models to help understand the relationships and make predictions about diabetes probabilities in the future. The three models will be a logistic regression model, classification tree, and a random forest.\n#Data: Use a relative path to import data\n\n#Read in data and create a tibble \ndiabetes_data &lt;- read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\ndiabetes_data &lt;- as_tibble(diabetes_data)\n\n#Create vectors for the factor variables \nDiabetes_labels &lt;- c(\"No Diabetes\", \"Diabetes\")\nHighBP_labels &lt;- c(\"Not High BP\", \"High BP\")\nHighChol_labels &lt;- c(\"Not High Chol\", \"Chol\")\nPhysActivity_labels &lt;- c(\"No\",\"Yes\")\nSex_labels &lt;- c(\"Female\",\"Male\")\nAge_labels &lt;- c(\"18-24\",\"25-29\",\"30-34\",\"35-59\",\n                \"40-44\",\"45-49\",\"50-54\",\"55-59\",\n                \"60-64\", \"65-69\", \"70-74\", \"75-79\",\"80 or older\")\nIncome_levels &lt;-c(\"Less than $10,000\",\"&gt;15000\",\"&gt;20000\",\"&gt;25000\",\"&gt;35000\"\n                  ,\"&gt;50000\",\"&gt;75000\",\"75000+\")\n\n\n\n#Mutating the data set to only include the relevant variables models ran and to \n#modify the type of variable and remove missing variables \ndiabetes_data &lt;- diabetes_data |&gt;\n  drop_na() |&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = Diabetes_labels),\n    HighBP = factor(HighBP, levels = c(0,1), labels = HighBP_labels),\n    HighChol = factor(HighChol, levels = c(0,1), labels = HighChol_labels),\n    PhysActivity = factor(PhysActivity, levels = c(0,1), labels = PhysActivity_labels),\n    Sex = factor(Sex, levels = c(0,1), labels = Sex_labels),\n    Age = factor(Age, levels = 1:13, labels = Age_labels),\n    Income = factor(Income, levels = 1:8, labels = Income_levels)\n  ) |&gt;\n  select(Diabetes_binary,HighBP,PhysActivity,BMI)\n\n#Summarizations: Univariate\n\n#Look at summary of the data and the structure of the variables \nsummary(diabetes_data)\nstr(diabetes_data)\n\nAbove, we can see the frequency of our variables and the variable type for each variable. This will be important later as we create the models. We will get more into the summary of each variable below.\n\n#Graph for BMI\nggplot(data=diabetes_data, aes(x=BMI))+\n  geom_histogram(aes(y=after_stat(count/sum(count))),\n                 binwidth=2,fill=\"lightblue\",\n                 color =\"blue\") +\n  scale_y_continuous(labels = function(x) paste0(round(x * 100, 1), \"%\")) +\n  xlab(\"BMI\")+\n  ylab(\"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nLooking at the histogram of BMI by count, we can determine the data is slightly left skewed, while the median BMI in the data set is 27.\n\n#Pivot  \nFrequencies &lt;- diabetes_data |&gt;\n  select(\"HighBP\", \"Diabetes_binary\", \"PhysActivity\") |&gt;\n  pivot_longer(everything(),names_to = \"Variables\", values_to=\"Categories\") |&gt;\n  group_by(Variables,Categories) |&gt;\n  summarize(count = n(), .groups =\"drop\") |&gt;\n  group_by(Variables) |&gt;\n  mutate(Percentage = round(100 * count / sum(count), 1))\n\n\n#Graph for Frequencies of Physical Activity, High BP, and Diabetes \nggplot(Frequencies, aes(x = factor(Categories), y = Percentage, fill = factor(Categories))) +\n  geom_col(color = \"blue\") +\n  geom_text(aes(label = paste0(Percentage, \"%\")), vjust = -0.5) +\n  scale_y_continuous(limits = c(0, 100),expand = expansion(mult = c(0, 0.05))) +\n  facet_wrap(~ Variables, scales = \"free_x\") +\n  labs(x = \"Category\", y = \"Percentage\", title = \"Distribution of Variables in Diabetes Dataset\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFrom looking at this chart, we can determine that only around 14% of our sample has diabetes.\nThe amount of the sample with high blood pressure is almost 43% and those who excercise is around 76%.\n#Look at relationship between diabetes and each of the #indepedent variables\n\n#Diabetes + BMI\nDiabetes_means &lt;- diabetes_data |&gt; \n  group_by(Diabetes_binary)|&gt;\n  summarize(BMI_mean = mean(BMI,na.rm =TRUE), count =n())\n            \nprint(Diabetes_means)\n\n# A tibble: 2 × 3\n  Diabetes_binary BMI_mean  count\n  &lt;fct&gt;              &lt;dbl&gt;  &lt;int&gt;\n1 No Diabetes         27.8 218334\n2 Diabetes            31.9  35346\n\nggplot(data = diabetes_data, aes(x = Diabetes_binary, y = BMI, fill = Diabetes_binary)) +\n  geom_boxplot(outlier.color = \"red\", outlier.shape = 16, outlier.size = 2, notch = TRUE) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, fill = \"yellow\", color = \"black\")+\n  scale_fill_manual(values = c(\"skyblue\", \"salmon\")) +\n  labs(\n    title = \"BMI Distribution by Diabetes Status\",\n    x = \"Diabetes Status\",\n    y = \"Body Mass Index (BMI)\",\n    fill = \"Diabetes\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\nFrom the summary table and the box plot, we can see that the mean BMI from those with diabetes is slightly greater than those without diabetes, concluding there could be some relationship that a higher BMI relates to having diabetes.\n\n#Diabetes + High BP\n#Create a frequency table \nHigh_BP_Percentages &lt;- diabetes_data |&gt;\n    group_by(Diabetes_binary,HighBP) |&gt;\n  summarise(count =n (),.groups = 'drop') |&gt;\n  group_by(Diabetes_binary)|&gt;\n  mutate(percent = 100 * count / sum(count))\n\nprint(High_BP_Percentages)\n\n# A tibble: 4 × 4\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary HighBP       count percent\n  &lt;fct&gt;           &lt;fct&gt;        &lt;int&gt;   &lt;dbl&gt;\n1 No Diabetes     Not High BP 136109    62.3\n2 No Diabetes     High BP      82225    37.7\n3 Diabetes        Not High BP   8742    24.7\n4 Diabetes        High BP      26604    75.3\n\n#Create a stacked bar chart to visually demonstrate \nggplot(High_BP_Percentages, aes(x = factor(Diabetes_binary), y = percent, fill = HighBP)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  geom_text(aes(label = paste0(round(percent, 1), \"%\")),\n            position = position_stack(vjust = 0.5), size = 4, color = \"black\") +\n  scale_fill_brewer(palette = \"Pastel1\") +\n  labs(\n    title = \"High BP by Diabetes Status\",\n    x = \"Diabetes Status\",\n    y = \"Percentage\",\n    fill = \"High Blood Pressure\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFrom this graph, we can clearly see that those who have diabetes are much more likely to have high blood pressure as 75% have high blood pressure compared to only 38% of those who do not have diabetes.\n\n#Diabetes + High BP\n#Create a frequency table \nPhys_Activity_Percentages &lt;- diabetes_data |&gt;\n    group_by(Diabetes_binary,PhysActivity) |&gt;\n  summarise(count =n (),.groups = 'drop') |&gt;\n  group_by(Diabetes_binary)|&gt;\n  mutate(percent = 100 * count / sum(count))\n\nprint(Phys_Activity_Percentages)\n\n# A tibble: 4 × 4\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary PhysActivity  count percent\n  &lt;fct&gt;           &lt;fct&gt;         &lt;int&gt;   &lt;dbl&gt;\n1 No Diabetes     No            48701    22.3\n2 No Diabetes     Yes          169633    77.7\n3 Diabetes        No            13059    36.9\n4 Diabetes        Yes           22287    63.1\n\n#Create a stacked bar chart to visually demonstrate \nggplot(Phys_Activity_Percentages, aes(x = factor(Diabetes_binary), y = percent, fill = PhysActivity)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  geom_text(aes(label = paste0(round(percent, 1), \"%\")),\n            position = position_stack(vjust = 0.5), size = 4, color = \"black\") +\n  scale_fill_brewer(palette = \"Pastel1\") +\n  labs(\n    title = \"Physical Activity by Diabetes Status\",\n    x = \"Diabetes Status\",\n    y = \"Percentage\",\n    fill = \"Physical Activity\"\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nWhen looking at the chart of diabetes status by physical activity, we can see that while over 50% of those with diabetes do physical activity, those who do not have diabetes do more physical activity.\n\n#Looking interactions between the BMI and High blood pressure\nBMIMeans &lt;- diabetes_data |&gt;\n  group_by(HighBP, Diabetes_binary) |&gt;\n  summarize(mean_BMI =mean(BMI),.groups=\"drop\")\n\nprint(BMIMeans)\n\n# A tibble: 4 × 3\n  HighBP      Diabetes_binary mean_BMI\n  &lt;fct&gt;       &lt;fct&gt;              &lt;dbl&gt;\n1 Not High BP No Diabetes         26.9\n2 Not High BP Diabetes            30.4\n3 High BP     No Diabetes         29.2\n4 High BP     Diabetes            32.4\n\nggplot(data = diabetes_data, aes(x = HighBP, y = BMI, fill = factor(Diabetes_binary))) +\n  geom_boxplot(outlier.color = \"red\", outlier.size = 1) +\n  scale_fill_manual(\n    values = c(\"No Diabetes\" = \"skyblue\", \"Diabetes\" = \"salmon\"),\n    name = \"Diabetes Status\",\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    title = \"BMI Distribution by High Blood Pressure and Diabetes Status\",\n    x = \"High Blood Pressure\",\n    y = \"Body Mass Index (BMI)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\nFrom the table of means and the boxplot we can determine that those with diabetes have a slightly greater BMI despite there blood pressure status.\n\n#Looking interactions between the BMI and Physical Activity\nPhysicalAct_Means &lt;- diabetes_data |&gt;\n  group_by(PhysActivity, Diabetes_binary) |&gt;\n  summarize(mean_BMI =mean(BMI),.groups=\"drop\")\n\nprint(PhysicalAct_Means)\n\n# A tibble: 4 × 3\n  PhysActivity Diabetes_binary mean_BMI\n  &lt;fct&gt;        &lt;fct&gt;              &lt;dbl&gt;\n1 No           No Diabetes         29.3\n2 No           Diabetes            33.3\n3 Yes          No Diabetes         27.4\n4 Yes          Diabetes            31.2\n\nggplot(data = diabetes_data, aes(x = PhysActivity, y = BMI, fill = factor(Diabetes_binary))) +\n  geom_boxplot(outlier.color = \"red\", outlier.size = 1) +\n  scale_fill_manual(\n    values = c(\"No Diabetes\" = \"skyblue\", \"Diabetes\" = \"salmon\"),\n    name = \"Diabetes Status\",\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    title = \"BMI Distribution by Physical Activity and Diabetes Status\",\n    x = \"Physical Activity\",\n    y = \"Body Mass Index (BMI)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n#Takeaways from EDA\nFrom our EDA, we can determine that all three variables have at least some effect on diabetes, although it does not seem that all variables were created equal, as high blood seems to have the strongest relationship with diabetes.\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "#Basic Introduction\n#Split data into testing and training data\n#Introduction\nOn our EDA page, we explored three variables, BMI, High blood pressure, and physical activity, by looking at their distribution of each of the variables in the data set as well as their relationship with diabetes. These variables were picked to determine the cardiometabolic factors effect on diabetes.\nNow, we will fit three models, logistic regression model, classification tree, and random forests to look at how they relate with each other in terms of predicting the likelihood of having diabetes.\n#Split data into testing and training set\n\nset.seed(123)\n\nmodel_split &lt;- initial_split(diabetes_data,prop=.7)\ntest &lt;- testing(model_split)\ntrain &lt;- training(model_split)\ndiabetes_CV_folds &lt;- vfold_cv(train, 5)\n\n#Logistic Regression Model\nA logistic regression model is a model that can predict the probability of a binary variable based on various independent variables.\nIn this example, we can apply this model since our dependent variable, diabetes, is binary. We will fit three models, one with BMI, High blood pressure, and PhysActivity, one with those three variables and an interaction on BMI and High blood pressure, and one with the three variables and an interaction on BMI and physical activity.\n\n#Model 1: BMI HighBP and PhysActivity\nLR1 &lt;- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |&gt;\n  step_normalize(BMI)|&gt;\n  step_dummy(HighBP, PhysActivity)\n\n#Model 2: BMI HighBP and PhysActivity and interaction term between\n#BMI and highBP \nLR2 &lt;- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |&gt;\n  step_dummy(HighBP, PhysActivity)|&gt;\n  step_normalize(all_predictors())|&gt; \n  step_interact(~ BMI:starts_with(\"HighBP_\"))\n\n#Model 3: BMI HighBP and PhysActivity and interaction term between\n#BMI and PhysActivity \nLR3 &lt;- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |&gt;\n  step_dummy(HighBP, PhysActivity)|&gt;\n  step_normalize(all_predictors())|&gt; \n  step_interact(~ BMI:starts_with(\"PhysActivity_\"))\n\n#Set model type and engine \nLR_spec &lt;- logistic_reg() |&gt;\n set_engine(\"glm\")\n\n#Create workflows\nLR1_wkf &lt;- workflow() |&gt;\n add_recipe(LR1) |&gt;\n add_model(LR_spec)\nLR2_wkf &lt;- workflow() |&gt;\n add_recipe(LR2) |&gt;\n add_model(LR_spec)\nLR3_wkf &lt;- workflow() |&gt;\n add_recipe(LR3) |&gt;\n add_model(LR_spec)\n\n#Fit to CV folds \nLR1_fit &lt;- LR1_wkf |&gt;\n fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))\nLR2_fit &lt;- LR2_wkf |&gt;\n fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))\nLR3_fit &lt;- LR3_wkf |&gt;\n fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))\n\nrbind(LR1_fit |&gt; collect_metrics(),\n LR2_fit |&gt; collect_metrics(),\n LR3_fit |&gt; collect_metrics()) |&gt;\n mutate(Model = c(\"Model1\", \"Model1\", \"Model2\", \"Model2\", \"Model3\", \"Model3\")) |&gt;\n select(Model, everything())\n\n# A tibble: 6 × 7\n  Model  .metric     .estimator  mean     n  std_err .config             \n  &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 Model1 accuracy    binary     0.859     5 0.000420 Preprocessor1_Model1\n2 Model1 mn_log_loss binary     0.354     5 0.000741 Preprocessor1_Model1\n3 Model2 accuracy    binary     0.859     5 0.000397 Preprocessor1_Model1\n4 Model2 mn_log_loss binary     0.354     5 0.000728 Preprocessor1_Model1\n5 Model3 accuracy    binary     0.859     5 0.000322 Preprocessor1_Model1\n6 Model3 mn_log_loss binary     0.354     5 0.000747 Preprocessor1_Model1\n\nmean(train$Diabetes_binary==\"Diabetes\")\n\n[1] 0.1390447\n\n\nFrom the infomation above, we will choose Model3 as our best model given the lowest log loss.\n#Classification Tree\nA classification tree can be used to predict the outcome of a categorical or binary variable by assigning variables to different classes. This is done by using a fitting process and partitioning by the specific variables into separate sections. This can be easier to interpret and better for non-linear data than a logistic regression model would be.\nIn this model we will take BMI, high blood pressure and physical activity and using the fitting process, determine specific branches to find groups that are more likely to have diabetes.\n\n#Create Recipe \ntree_rec &lt;- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |&gt;\n  step_normalize(BMI)|&gt;\n  step_dummy(all_nominal_predictors())\n\ntree_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: BMI\n\n\n• Dummy variables from: all_nominal_predictors()\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n  min_n = 20,\n  cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\n#Create workflow \ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_mod)\n\n#Set up my own tuning grid\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\ntree_fits &lt;- tree_wkf |&gt; \n  tune_grid(resamples = diabetes_CV_folds,\n            grid = tree_grid,\n            metrics=metric_set(accuracy, mn_log_loss))\n\ntree_fits |&gt;\n  collect_metrics() \n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 accuracy    binary     0.861     5 3.80e-4 Prepro…\n 2    0.0000000001          1 mn_log_loss binary     0.403     5 6.93e-4 Prepro…\n 3    0.000000001           1 accuracy    binary     0.861     5 3.80e-4 Prepro…\n 4    0.000000001           1 mn_log_loss binary     0.403     5 6.93e-4 Prepro…\n 5    0.00000001            1 accuracy    binary     0.861     5 3.80e-4 Prepro…\n 6    0.00000001            1 mn_log_loss binary     0.403     5 6.93e-4 Prepro…\n 7    0.0000001             1 accuracy    binary     0.861     5 3.80e-4 Prepro…\n 8    0.0000001             1 mn_log_loss binary     0.403     5 6.93e-4 Prepro…\n 9    0.000001              1 accuracy    binary     0.861     5 3.80e-4 Prepro…\n10    0.000001              1 mn_log_loss binary     0.403     5 6.93e-4 Prepro…\n# ℹ 90 more rows\n\ntree_fits %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001         11 mn_log_loss binary     0.356     5 7.45e-4 Prepro…\n 2    0.000000001          11 mn_log_loss binary     0.356     5 7.45e-4 Prepro…\n 3    0.00000001           11 mn_log_loss binary     0.356     5 7.45e-4 Prepro…\n 4    0.0000001            11 mn_log_loss binary     0.356     5 7.45e-4 Prepro…\n 5    0.000001             11 mn_log_loss binary     0.356     5 7.45e-4 Prepro…\n 6    0.0000000001          8 mn_log_loss binary     0.356     5 7.49e-4 Prepro…\n 7    0.000000001           8 mn_log_loss binary     0.356     5 7.49e-4 Prepro…\n 8    0.00000001            8 mn_log_loss binary     0.356     5 7.49e-4 Prepro…\n 9    0.0000001             8 mn_log_loss binary     0.356     5 7.49e-4 Prepro…\n10    0.000001              8 mn_log_loss binary     0.356     5 7.49e-4 Prepro…\n# ℹ 40 more rows\n\ntree_best_params &lt;- select_best(tree_fits, metric = \"mn_log_loss\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         11 Preprocessor1_Model31\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(model_split,metrics = metric_set(accuracy, mn_log_loss))\ntree_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177576/76104]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.357 Preprocessor1_Model1\n\n\n#Random Forest\nA random forest model is very similar to a classification tree as it similarly predicts the outcome of a categorical variable by splitting each variable into subsets to come to a final decision/ predictions. The difference between the classification tree and a random forest is that a random forest is an ensemble model. This means that this will produce multiple decision trees and will take the average across all of the trees. The classification model will only ever produce one tree. The purpose of this is to reduce over fitting.\n\nran_forest_rec &lt;- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |&gt;\n  step_normalize(BMI)|&gt;\n  step_dummy(all_nominal_predictors())\n\nran_forest_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: BMI\n\n\n• Dummy variables from: all_nominal_predictors()\n\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n set_engine(\"ranger\") |&gt;\n set_mode(\"classification\")\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(ran_forest_rec) |&gt;\n add_model(rf_spec)\n\nrf_fit &lt;- rf_wkf |&gt;\n tune_grid(resamples = diabetes_CV_folds,\n grid = 7,\n metrics = metric_set(accuracy, mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_fit |&gt;\n collect_metrics() |&gt;\n filter(.metric == \"mn_log_loss\") |&gt;\n arrange(mean)\n\n# A tibble: 3 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     2 mn_log_loss binary     0.351     5 0.000626 Preprocessor1_Model3\n2     3 mn_log_loss binary     0.354     5 0.000818 Preprocessor1_Model1\n3     1 mn_log_loss binary     0.356     5 0.000734 Preprocessor1_Model2\n\nrf_best_params &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     2 Preprocessor1_Model3\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n finalize_workflow(rf_best_params)\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(model_split, metrics = metric_set(accuracy, mn_log_loss))\n\n#Final Model Selection\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(model_split,metrics = metric_set(accuracy, mn_log_loss))\ntree_final_fit|&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.357 Preprocessor1_Model1\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(model_split, metrics = metric_set(accuracy, mn_log_loss))\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.352 Preprocessor1_Model1\n\nLR3_final_fit &lt;- LR3_wkf |&gt;\n last_fit(model_split, metrics = metric_set(accuracy, mn_log_loss)) \n\ntree_final_fit|&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.357 Preprocessor1_Model1\n\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.352 Preprocessor1_Model1\n\nLR3_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.859 Preprocessor1_Model1\n2 mn_log_loss binary         0.356 Preprocessor1_Model1\n\nrbind(tree_final_fit |&gt; collect_metrics(),\nrf_final_fit |&gt; collect_metrics(),\n LR3_final_fit |&gt; collect_metrics()) |&gt;\n mutate(Model = c(\"Classification Model\", \"Classification Model\", \"Random Forest\", \"Random Forest\", \"Logistic Regression\", \"Logistic Regression\")) |&gt;\n select(Model, everything())\n\n# A tibble: 6 × 5\n  Model                .metric     .estimator .estimate .config             \n  &lt;chr&gt;                &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Classification Model accuracy    binary         0.860 Preprocessor1_Model1\n2 Classification Model mn_log_loss binary         0.357 Preprocessor1_Model1\n3 Random Forest        accuracy    binary         0.860 Preprocessor1_Model1\n4 Random Forest        mn_log_loss binary         0.352 Preprocessor1_Model1\n5 Logistic Regression  accuracy    binary         0.859 Preprocessor1_Model1\n6 Logistic Regression  mn_log_loss binary         0.356 Preprocessor1_Model1\n\nfinal_model &lt;- rf_final_fit$.workflow[[1]]\n\nAbove, we have the best model of the logistic regression model, classification tree, and random forest. Using log loss, we have determined the best model of all 3 model choices and made a dataframe to compare them against each other.\nBy looking at the log loss metrics, we can see that random forest has the lowest value for log-loss and is the best model overall of the three that were fit."
  }
]