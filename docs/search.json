[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n#Introduction section\n##Briefly describes the data and variables you have to work with\n##Describe the purpose of your EDA and ultimate goal of modeling\n#Data: Use a relative path to import data\n\n#Read in data and create a tibble \ndiabetes_data &lt;- read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\ndiabetes_data &lt;- as_tibble(diabetes_data)\n\n#Create vectors for the factor variables \nDiabetes_labels &lt;- c(\"No Diabetes\", \"Diabetes\")\nHighBP_labels &lt;- c(\"Not High BP\", \"High BP\")\nHighChol_labels &lt;- c(\"Not High Chol\", \"Chol\")\nPhysActivity_labels &lt;- c(\"No\",\"Yes\")\nSex_labels &lt;- c(\"Female\",\"Male\")\nAge_labels &lt;- c(\"18-24\",\"25-29\",\"30-34\",\"35-59\",\n                \"40-44\",\"45-49\",\"50-54\",\"55-59\",\n                \"60-64\", \"65-69\", \"70-74\", \"75-79\",\"80 or older\")\nIncome_levels &lt;-c(\"Less than $10,000\",\"&gt;15000\",\"&gt;20000\",\"&gt;25000\",\"&gt;35000\",\"&gt;50000\",\n                  \"&gt;75000\",\"75000+\")\n\n\n\n#Mutating the data set to only include the relevant variables models ran and to modify \n#the type of variable \ndiabetes_data &lt;- diabetes_data |&gt;\n  drop_na() |&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = Diabetes_labels),\n    HighBP = factor(HighBP, levels = c(0,1), labels = HighBP_labels),\n    HighChol = factor(HighChol, levels = c(0,1), labels = HighChol_labels),\n    PhysActivity = factor(PhysActivity, levels = c(0,1), labels = PhysActivity_labels),\n    Sex = factor(Sex, levels = c(0,1), labels = Sex_labels),\n    Age = factor(Age, levels = 1:13, labels = Age_labels),\n    Income = factor(Income, levels = 1:8, labels = Income_levels)\n  ) |&gt;\n  select(Diabetes_binary,HighBP,HighChol,PhysActivity,Sex,Age,Income,BMI)\n\nsummary(diabetes_data)\nstr(diabetes_data)\n\n#Summarizations: Univariate #Looking at summary statistics of each of the three variables #Across the whole data set\n\ndf_EDA &lt;- diabetes_data |&gt;\n  drop_na() |&gt;\n  select(Diabetes_binary, HighChol, BMI, PhysActivity,HighBP)\n\n#Obtain summary statistics\nsummary(df_EDA)\n\n    Diabetes_binary            HighChol           BMI        PhysActivity\n No Diabetes:218334   Not High Chol:146089   Min.   :12.00   No : 61760  \n Diabetes   : 35346   Chol         :107591   1st Qu.:24.00   Yes:191920  \n                                             Median :27.00               \n                                             Mean   :28.38               \n                                             3rd Qu.:31.00               \n                                             Max.   :98.00               \n         HighBP      \n Not High BP:144851  \n High BP    :108829  \n                     \n                     \n                     \n                     \n\nstr(df_EDA)\n\ntibble [253,680 × 5] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary: Factor w/ 2 levels \"No Diabetes\",..: 1 1 1 1 1 1 1 1 2 1 ...\n $ HighChol       : Factor w/ 2 levels \"Not High Chol\",..: 2 1 2 1 2 2 1 2 2 1 ...\n $ BMI            : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ PhysActivity   : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ HighBP         : Factor w/ 2 levels \"Not High BP\",..: 2 1 2 2 2 2 2 2 2 1 ...\n\n#Graph for BMI\nggplot(data=df_EDA, aes(x=BMI))+\n  geom_histogram(binwidth=2)\n\n\n\n\n\n\n\n#Graph for High Blood Pressure Distribution \nggplot(data=df_EDA, aes(x=HighBP)) +\n  geom_bar() +\n  geom_text(stat = \"count\", aes(label = after_stat(count)), vjust = -0.5) +\n  labs(title =\"High Blood Pressure Distribution\", x=\"High Blood Pressure\", y=\"Count\")+\n  scale_x_discrete(labels =c(\"No\",\n                             \"Yes\"))\n\n\n\n\n\n\n\n#Graph for Physical Activity \nggplot(data=df_EDA, aes(x=PhysActivity)) +\n  geom_bar() +\n  labs(title =\"Physical Activity Distribution\", x=\"Physical Activity\", y=\"Count\")+\n  scale_x_discrete(labels =c(\"No\",\n                             \"Yes\"))\n\n\n\n\n\n\n\n#Graph for Diabetes \nggplot(data=df_EDA, aes(x=Diabetes_binary)) +\n  geom_bar() +\n  labs(title =\"Diabetes Distribution\", x=\"Diabetes\", y=\"Count\")+\n  scale_x_discrete(labels =c(\"No\",\n                             \"Yes\"))\n\n\n\n\n\n\n\n\n#Look at relationship between diabetes and each of the #indepedent variables\n\n#Diabetes + BMI\nggplot(data = df_EDA) + \n  geom_boxplot(mapping = aes(x = Diabetes_binary, y = BMI))\n\n\n\n\n\n\n\n#Diabetes + Physical Activity\nggplot(data=df_EDA, aes(fill = PhysActivity, x=Diabetes_binary)) +\n  geom_bar() \n\n\n\n\n\n\n\ntable(df_EDA$Diabetes_binary,df_EDA$PhysActivity)\n\n             \n                  No    Yes\n  No Diabetes  48701 169633\n  Diabetes     13059  22287\n\n#Diabetes + High Blood Pressure\nggplot(data=df_EDA, aes(fill = HighBP, x=Diabetes_binary)) +\n  geom_bar() \n\n\n\n\n\n\n\ntable(df_EDA$Diabetes_binary,df_EDA$HighBP)\n\n             \n              Not High BP High BP\n  No Diabetes      136109   82225\n  Diabetes           8742   26604\n\n#Looking interactions between the BMI and High blood pressure\nggplot(data = df_EDA, \n  aes(x = HighBP, y = BMI, fill=Diabetes_binary))+\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n#Looking at interactions between BMI and physical activity\nggplot(data = df_EDA, \n  aes(x = PhysActivity, y = BMI, fill=Diabetes_binary))+\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n#Be sure to have a narrative about what you are exploring and what the summaries and graphs you created say about the relationships in your data.\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "#Basic Introduction\n#Split data into testing and training data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.9     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.1\n✔ parsnip      1.3.2     ✔ yardstick    1.3.2\n✔ recipes      1.3.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(yardstick)\nlibrary(ggplot2)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-9\n\nlibrary(tree)\nlibrary(baguette)\nlibrary(parsnip)\nlibrary(ranger)\n\n\n#Reload the data \n#Read in data and create a tibble \ndiabetes_data &lt;- read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\ndiabetes_data &lt;- as_tibble(diabetes_data)\n\n#Create vectors for the factor variables \nDiabetes_labels &lt;- c(\"No Diabetes\", \"Diabetes\")\nHighBP_labels &lt;- c(\"Not High BP\", \"High BP\")\nHighChol_labels &lt;- c(\"Not High Chol\", \"Chol\")\nPhysActivity_labels &lt;- c(\"No\",\"Yes\")\nSex_labels &lt;- c(\"Female\",\"Male\")\nAge_labels &lt;- c(\"18-24\",\"25-29\",\"30-34\",\"35-59\",\n                \"40-44\",\"45-49\",\"50-54\",\"55-59\",\n                \"60-64\", \"65-69\", \"70-74\", \"75-79\",\"80 or older\")\nIncome_levels &lt;-c(\"Less than $10,000\",\"&gt;15000\",\"&gt;20000\",\"&gt;25000\",\"&gt;35000\",\"&gt;50000\",\n                  \"&gt;75000\",\"75000+\")\n\n\n\n#Mutating the data set to only include the relevant variables models ran and to modify \n#the type of variable \ndiabetes_data &lt;- diabetes_data |&gt;\n  drop_na() |&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = Diabetes_labels),\n    HighBP = factor(HighBP, levels = c(0,1), labels = HighBP_labels),\n    HighChol = factor(HighChol, levels = c(0,1), labels = HighChol_labels),\n    PhysActivity = factor(PhysActivity, levels = c(0,1), labels = PhysActivity_labels),\n    Sex = factor(Sex, levels = c(0,1), labels = Sex_labels),\n    Age = factor(Age, levels = 1:13, labels = Age_labels),\n    Income = factor(Income, levels = 1:8, labels = Income_levels)\n  ) |&gt;\n  select(Diabetes_binary,HighBP,HighChol,PhysActivity,Sex,Age,Income,BMI)\n\n\nset.seed(123)\n\nmodel_split &lt;- initial_split(diabetes_data,prop=.7)\ntest &lt;- testing(model_split)\ntrain &lt;- training(model_split)\ndiabetes_CV_folds &lt;- vfold_cv(train, 5)\n\n#Logistic Regression Model\n\n#Model 1: BMI HighBP and PhysActivity\nLR1 &lt;- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |&gt;\n  step_normalize(BMI)|&gt;\n  step_dummy(HighBP, PhysActivity)\n\n#Model 2: BMI HighBP and PhysActivity and interaction term between\n#BMI and highBP \nLR2 &lt;- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |&gt;\n  step_dummy(HighBP, PhysActivity)|&gt;\n  step_normalize(all_predictors())|&gt; \n  step_interact(~ BMI:starts_with(\"HighBP_\"))\n\n#Model 3: BMI HighBP and PhysActivity and interaction term between\n#BMI and PhysActivity \nLR3 &lt;- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |&gt;\n  step_dummy(HighBP, PhysActivity)|&gt;\n  step_normalize(all_predictors())|&gt; \n  step_interact(~ BMI:starts_with(\"PhysActivity_\"))\n\n#Set model type and engine \nLR_spec &lt;- logistic_reg() |&gt;\n set_engine(\"glm\")\n\n#Create workflows\nLR1_wkf &lt;- workflow() |&gt;\n add_recipe(LR1) |&gt;\n add_model(LR_spec)\nLR2_wkf &lt;- workflow() |&gt;\n add_recipe(LR2) |&gt;\n add_model(LR_spec)\nLR3_wkf &lt;- workflow() |&gt;\n add_recipe(LR3) |&gt;\n add_model(LR_spec)\n\n#Fit to CV folds \nLR1_fit &lt;- LR1_wkf |&gt;\n fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))\nLR2_fit &lt;- LR2_wkf |&gt;\n fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))\nLR3_fit &lt;- LR3_wkf |&gt;\n fit_resamples(diabetes_CV_folds, metrics = metric_set(accuracy, mn_log_loss))\n\nrbind(LR1_fit |&gt; collect_metrics(),\n LR2_fit |&gt; collect_metrics(),\n LR3_fit |&gt; collect_metrics()) |&gt;\n mutate(Model = c(\"Model1\", \"Model1\", \"Model2\", \"Model2\", \"Model3\", \"Model3\")) |&gt;\n select(Model, everything())\n\n# A tibble: 6 × 7\n  Model  .metric     .estimator  mean     n  std_err .config             \n  &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 Model1 accuracy    binary     0.859     5 0.000420 Preprocessor1_Model1\n2 Model1 mn_log_loss binary     0.354     5 0.000741 Preprocessor1_Model1\n3 Model2 accuracy    binary     0.859     5 0.000397 Preprocessor1_Model1\n4 Model2 mn_log_loss binary     0.354     5 0.000728 Preprocessor1_Model1\n5 Model3 accuracy    binary     0.859     5 0.000322 Preprocessor1_Model1\n6 Model3 mn_log_loss binary     0.354     5 0.000747 Preprocessor1_Model1\n\nmean(train$Diabetes_binary==\"Diabetes\")\n\n[1] 0.1390447\n\n\n#Classification Tree\n\n#Create Recipe \ntree_rec &lt;- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |&gt;\n  step_normalize(BMI)|&gt;\n  step_dummy(all_nominal_predictors())\n\ntree_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: BMI\n\n\n• Dummy variables from: all_nominal_predictors()\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n  min_n = 20,\n  cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\n#Create workflow \ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_mod)\n\n#Set up my own tuning grid\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\ntree_fits &lt;- tree_wkf |&gt; \n  tune_grid(resamples = diabetes_CV_folds,\n            grid = tree_grid,\n            metrics=metric_set(accuracy, mn_log_loss))\n\ntree_fits |&gt;\n  collect_metrics() \n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 accuracy    binary     0.861     5 3.80e-4 Prepro…\n 2    0.0000000001          1 mn_log_loss binary     0.403     5 6.93e-4 Prepro…\n 3    0.000000001           1 accuracy    binary     0.861     5 3.80e-4 Prepro…\n 4    0.000000001           1 mn_log_loss binary     0.403     5 6.93e-4 Prepro…\n 5    0.00000001            1 accuracy    binary     0.861     5 3.80e-4 Prepro…\n 6    0.00000001            1 mn_log_loss binary     0.403     5 6.93e-4 Prepro…\n 7    0.0000001             1 accuracy    binary     0.861     5 3.80e-4 Prepro…\n 8    0.0000001             1 mn_log_loss binary     0.403     5 6.93e-4 Prepro…\n 9    0.000001              1 accuracy    binary     0.861     5 3.80e-4 Prepro…\n10    0.000001              1 mn_log_loss binary     0.403     5 6.93e-4 Prepro…\n# ℹ 90 more rows\n\ntree_fits %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001         11 mn_log_loss binary     0.356     5 7.45e-4 Prepro…\n 2    0.000000001          11 mn_log_loss binary     0.356     5 7.45e-4 Prepro…\n 3    0.00000001           11 mn_log_loss binary     0.356     5 7.45e-4 Prepro…\n 4    0.0000001            11 mn_log_loss binary     0.356     5 7.45e-4 Prepro…\n 5    0.000001             11 mn_log_loss binary     0.356     5 7.45e-4 Prepro…\n 6    0.0000000001          8 mn_log_loss binary     0.356     5 7.49e-4 Prepro…\n 7    0.000000001           8 mn_log_loss binary     0.356     5 7.49e-4 Prepro…\n 8    0.00000001            8 mn_log_loss binary     0.356     5 7.49e-4 Prepro…\n 9    0.0000001             8 mn_log_loss binary     0.356     5 7.49e-4 Prepro…\n10    0.000001              8 mn_log_loss binary     0.356     5 7.49e-4 Prepro…\n# ℹ 40 more rows\n\ntree_best_params &lt;- select_best(tree_fits, metric = \"mn_log_loss\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         11 Preprocessor1_Model31\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(model_split,metrics = metric_set(accuracy, mn_log_loss))\ntree_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177576/76104]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.357 Preprocessor1_Model1\n\n\n#Random Forest\n\nran_forest_rec &lt;- recipe(Diabetes_binary ~ BMI + HighBP + PhysActivity, data=train) |&gt;\n  step_normalize(BMI)|&gt;\n  step_dummy(all_nominal_predictors())\n\nran_forest_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: BMI\n\n\n• Dummy variables from: all_nominal_predictors()\n\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n set_engine(\"ranger\") |&gt;\n set_mode(\"classification\")\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(ran_forest_rec) |&gt;\n add_model(rf_spec)\n\nrf_fit &lt;- rf_wkf |&gt;\n tune_grid(resamples = diabetes_CV_folds,\n grid = 7,\n metrics = metric_set(accuracy, mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_fit |&gt;\n collect_metrics() |&gt;\n filter(.metric == \"mn_log_loss\") |&gt;\n arrange(mean)\n\n# A tibble: 3 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     2 mn_log_loss binary     0.351     5 0.000626 Preprocessor1_Model3\n2     3 mn_log_loss binary     0.354     5 0.000818 Preprocessor1_Model1\n3     1 mn_log_loss binary     0.356     5 0.000734 Preprocessor1_Model2\n\nrf_best_params &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     2 Preprocessor1_Model3\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n finalize_workflow(rf_best_params)\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(model_split, metrics = metric_set(accuracy, mn_log_loss))\n\n#Final Model Selection\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(model_split,metrics = metric_set(accuracy, mn_log_loss))\ntree_final_fit|&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.357 Preprocessor1_Model1\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n last_fit(model_split, metrics = metric_set(accuracy, mn_log_loss))\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.352 Preprocessor1_Model1\n\nLR3_final_fit &lt;- LR3_wkf |&gt;\n last_fit(model_split, metrics = metric_set(accuracy, mn_log_loss)) \n\ntree_final_fit|&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.357 Preprocessor1_Model1\n\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.860 Preprocessor1_Model1\n2 mn_log_loss binary         0.352 Preprocessor1_Model1\n\nLR3_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.859 Preprocessor1_Model1\n2 mn_log_loss binary         0.356 Preprocessor1_Model1\n\nrbind(tree_final_fit |&gt; collect_metrics(),\nrf_final_fit |&gt; collect_metrics(),\n LR3_final_fit |&gt; collect_metrics()) |&gt;\n mutate(Model = c(\"Classification Model\", \"Classification Model\", \"Random Forest\", \"Random Forest\", \"Logistic Regression\", \"Logistic Regression\")) |&gt;\n select(Model, everything())\n\n# A tibble: 6 × 5\n  Model                .metric     .estimator .estimate .config             \n  &lt;chr&gt;                &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Classification Model accuracy    binary         0.860 Preprocessor1_Model1\n2 Classification Model mn_log_loss binary         0.357 Preprocessor1_Model1\n3 Random Forest        accuracy    binary         0.860 Preprocessor1_Model1\n4 Random Forest        mn_log_loss binary         0.352 Preprocessor1_Model1\n5 Logistic Regression  accuracy    binary         0.859 Preprocessor1_Model1\n6 Logistic Regression  mn_log_loss binary         0.356 Preprocessor1_Model1\n\nfinal_model &lt;- rf_final_fit$.workflow[[1]]\n\nDeclare Random Forest the Overall winner!!"
  }
]